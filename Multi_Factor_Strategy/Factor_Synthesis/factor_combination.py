# -*- coding: utf-8 -*-
"""
ä¸­è¯1000å¤šå› å­ç­–ç•¥ - å› å­åˆæˆæ¨¡å—
å®ç°å¤šæ¨¡å‹é›†æˆçš„å› å­åˆæˆæ–¹æ³•
"""

import numpy as np
import pandas as pd
import os
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# æ€§èƒ½ä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†å’Œè¿›åº¦æ¡
try:
    from joblib import Parallel, delayed
    from tqdm import tqdm
    JOBLIB_AVAILABLE = True
    TQDM_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False
    TQDM_AVAILABLE = False
    # å®šä¹‰å ä½å‡½æ•°
    def tqdm(iterable, *args, **kwargs):
        return iterable

# æ£€æŸ¥GPUå¯ç”¨æ€§
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    print(f"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
else:
    print("â„¹ï¸  ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")


class CCCLoss(nn.Module):
    """
    ä¸€è‡´æ€§ç›¸å…³ç³»æ•°ï¼ˆConcordance Correlation Coefficientï¼‰æŸå¤±å‡½æ•°
    CCC = (2 * rho * sigma_x * sigma_y) / (sigma_x^2 + sigma_y^2 + (mu_x - mu_y)^2)
    å…¶ä¸­ rho æ˜¯çš®å°”é€Šç›¸å…³ç³»æ•°
    """
    def __init__(self):
        super(CCCLoss, self).__init__()
    
    def forward(self, y_pred, y_true):
        """
        è®¡ç®—CCCæŸå¤±ï¼ˆè¿”å›1-CCCï¼Œå› ä¸ºæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±ï¼‰
        
        Parameters:
        -----------
        y_pred : torch.Tensor, é¢„æµ‹å€¼
        y_true : torch.Tensor, çœŸå®å€¼
        
        Returns:
        --------
        torch.Tensor : CCCæŸå¤±å€¼
        """
        # å»é™¤NaNå€¼
        mask = ~(torch.isnan(y_pred) | torch.isnan(y_true))
        if mask.sum() == 0:
            return torch.tensor(1.0, requires_grad=True)
        
        y_pred_clean = y_pred[mask]
        y_true_clean = y_true[mask]
        
        if len(y_pred_clean) < 2:
            return torch.tensor(1.0, requires_grad=True)
        
        # è®¡ç®—å‡å€¼
        mu_pred = torch.mean(y_pred_clean)
        mu_true = torch.mean(y_true_clean)
        
        # è®¡ç®—æ ‡å‡†å·®
        sigma_pred = torch.std(y_pred_clean)
        sigma_true = torch.std(y_true_clean)
        
        # è®¡ç®—åæ–¹å·®
        cov = torch.mean((y_pred_clean - mu_pred) * (y_true_clean - mu_true))
        
        # è®¡ç®—CCC
        denominator = sigma_pred ** 2 + sigma_true ** 2 + (mu_pred - mu_true) ** 2
        if denominator < 1e-8:
            return torch.tensor(1.0, requires_grad=True)
        
        ccc = 2 * cov / denominator
        
        # è¿”å›1-CCCä½œä¸ºæŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–CCCï¼Œå³æœ€å°åŒ–1-CCCï¼‰
        return 1 - ccc


def calculate_ccc_numpy(y_pred, y_true):
    """
    ä½¿ç”¨numpyè®¡ç®—CCCï¼ˆç”¨äºXGBoostç­‰éPyTorchæ¨¡å‹ï¼‰
    
    Parameters:
    -----------
    y_pred : np.ndarray, é¢„æµ‹å€¼
    y_true : np.ndarray, çœŸå®å€¼
    
    Returns:
    --------
    float : CCCå€¼
    """
    # å»é™¤NaNå€¼
    mask = ~(np.isnan(y_pred) | np.isnan(y_true))
    if mask.sum() < 2:
        return 0.0
    
    y_pred_clean = y_pred[mask]
    y_true_clean = y_true[mask]
    
    # è®¡ç®—å‡å€¼
    mu_pred = np.mean(y_pred_clean)
    mu_true = np.mean(y_true_clean)
    
    # è®¡ç®—æ ‡å‡†å·®
    sigma_pred = np.std(y_pred_clean)
    sigma_true = np.std(y_true_clean)
    
    # è®¡ç®—åæ–¹å·®
    cov = np.mean((y_pred_clean - mu_pred) * (y_true_clean - mu_true))
    
    # è®¡ç®—CCC
    denominator = sigma_pred ** 2 + sigma_true ** 2 + (mu_pred - mu_true) ** 2
    if denominator < 1e-8:
        return 0.0
    
    ccc = 2 * cov / denominator
    return ccc


class FactorDataset(Dataset):
    """å› å­æ•°æ®é›†ç±»"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class MLPModel(nn.Module):
    """å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹"""
    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3):
        super(MLPModel, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, 1))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x).squeeze()


class FactorCombiner:
    """å› å­åˆæˆå™¨ç±»"""
    
    def __init__(self, factor_data, return_data, constituent_manager=None):
        """
        åˆå§‹åŒ–å› å­åˆæˆå™¨
        
        Parameters:
        -----------
        factor_data : dict, å› å­æ•°æ®å­—å…¸ï¼Œkeyä¸ºå› å­åï¼Œvalueä¸ºDataFrameï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        return_data : pd.DataFrame, æ”¶ç›Šç‡æ•°æ®ï¼Œæ ¼å¼åŒfactor_dataï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        constituent_manager : ConstituentManager, æˆåˆ†è‚¡ç®¡ç†å™¨ï¼Œç”¨äºè¿‡æ»¤æˆåˆ†è‚¡
        """
        self.factor_data = factor_data
        self.return_data = return_data
        self.constituent_manager = constituent_manager
        
        # å¯¹é½æ•°æ®
        self._align_data()
        
        # å­˜å‚¨æ¨¡å‹é¢„æµ‹ç»“æœ
        self.model1_prediction = None  # å› å­ç­›é€‰ç­‰æƒåˆæˆ
        self.model2_prediction = None  # MLPæ¨¡å‹
        self.model3_prediction = None  # XGBoostæ¨¡å‹
        self.final_signal = None  # æœ€ç»ˆä¿¡å·
    
    def _align_data(self):
        """å¯¹é½å› å­æ•°æ®å’Œæ”¶ç›Šç‡æ•°æ®"""
        print("å¯¹é½å› å­æ•°æ®å’Œæ”¶ç›Šç‡æ•°æ®...")
        
        # è·å–æ‰€æœ‰å› å­çš„å…¬å…±æ—¥æœŸå’Œè‚¡ç¥¨
        all_dates = set(self.return_data.columns)
        all_stocks = set(self.return_data.index)
        
        for factor_name, factor_df in self.factor_data.items():
            if factor_df is None or factor_df.empty:
                continue
            all_dates = all_dates & set(factor_df.columns)
            all_stocks = all_stocks & set(factor_df.index)
        
        # å¯¹é½æ”¶ç›Šç‡æ•°æ®
        self.return_data = self.return_data.loc[list(all_stocks), list(sorted(all_dates))]
        
        # å¯¹é½å› å­æ•°æ®
        aligned_factors = {}
        for factor_name, factor_df in self.factor_data.items():
            if factor_df is None or factor_df.empty:
                aligned_factors[factor_name] = pd.DataFrame(
                    index=list(all_stocks), 
                    columns=list(sorted(all_dates))
                )
                aligned_factors[factor_name][:] = np.nan
            else:
                aligned = factor_df.loc[list(all_stocks), list(sorted(all_dates))]
                aligned_factors[factor_name] = aligned
        
        self.factor_data = aligned_factors
        print(f"âœ… æ•°æ®å¯¹é½å®Œæˆï¼Œè‚¡ç¥¨æ•°: {len(all_stocks)}, æ—¥æœŸæ•°: {len(all_dates)}")
    
    def _prepare_training_data(self, forward_period=1):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²
        
        Parameters:
        -----------
        forward_period : int, å‰ç»æœŸæ•°ï¼ˆé»˜è®¤1ï¼Œå³é¢„æµ‹ä¸‹æœŸæ”¶ç›Šç‡ï¼‰
        
        Returns:
        --------
        X : np.ndarray, ç‰¹å¾çŸ©é˜µ (æ ·æœ¬æ•° Ã— å› å­æ•°)
        y : np.ndarray, ç›®æ ‡å€¼ (æ ·æœ¬æ•°,)
        stock_info : list, æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„è‚¡ç¥¨å’Œæ—¥æœŸä¿¡æ¯
        """
        dates = sorted(self.return_data.columns)
        all_factors = list(self.factor_data.keys())
        
        X_list = []
        y_list = []
        stock_info = []
        
        # é¢„è®¡ç®—æˆåˆ†è‚¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        constituents_cache = {}
        if self.constituent_manager is not None:
            all_dates_set = set(dates)
            for date in all_dates_set:
                constituents_cache[date] = set(self.constituent_manager.get_constituents_by_date(date))
        
        # å‘é‡åŒ–å¤„ç†ï¼šæ‰¹é‡å¤„ç†æ—¥æœŸ
        iterator = tqdm(enumerate(dates), total=len(dates), desc="å‡†å¤‡è®­ç»ƒæ•°æ®") if TQDM_AVAILABLE else enumerate(dates)
        
        for i, date in iterator:
            if i + forward_period >= len(dates):
                break
            
            # å½“æœŸå› å­å€¼ï¼ˆtæ—¥ï¼‰
            factor_values_dict = {}
            for factor_name in all_factors:
                if factor_name in self.factor_data:
                    factor_df = self.factor_data[factor_name]
                    if date in factor_df.columns:
                        factor_values_dict[factor_name] = factor_df[date]
            
            # æœªæ¥æ”¶ç›Šç‡ï¼ˆt+forward_periodæ—¥ï¼‰
            future_date = dates[i + forward_period]
            future_returns = self.return_data[future_date]
            
            # è·å–å…¬å…±è‚¡ç¥¨ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
            common_stocks = set(future_returns.index)
            for factor_values in factor_values_dict.values():
                common_stocks = common_stocks & set(factor_values.index)
            
            # å¦‚æœæä¾›äº†æˆåˆ†è‚¡ç®¡ç†å™¨ï¼Œåªä½¿ç”¨æˆåˆ†è‚¡
            if self.constituent_manager is not None:
                if date in constituents_cache:
                    common_stocks = common_stocks & constituents_cache[date]
            
            if len(common_stocks) == 0:
                continue
            
            common_stocks = sorted(list(common_stocks))
            
            # æ„å»ºç‰¹å¾çŸ©é˜µ
            # å¯¹äºæ¯ä¸ªæ—¥æœŸï¼Œåªä½¿ç”¨åœ¨è¯¥æ—¥æœŸæœ‰å€¼çš„å› å­
            available_factors = list(factor_values_dict.keys())
            
            if len(available_factors) == 0:
                continue
            
            # å‘é‡åŒ–è®¡ç®—å› å­å‡å€¼
            factor_means = {}
            common_stocks_list = list(common_stocks)
            for factor_name in all_factors:
                if factor_name in factor_values_dict:
                    factor_series = factor_values_dict[factor_name]
                    # å‘é‡åŒ–æå–å’Œè®¡ç®—å‡å€¼
                    common_values = factor_series.loc[common_stocks_list]
                    valid_values = common_values.dropna()
                    if len(valid_values) > 0:
                        factor_means[factor_name] = valid_values.mean()
                    else:
                        factor_means[factor_name] = 0.0
                else:
                    factor_means[factor_name] = 0.0
            
            # æ‰¹é‡æ„å»ºç‰¹å¾çŸ©é˜µï¼ˆå‘é‡åŒ–ï¼‰
            min_valid_ratio = 0.3
            min_valid_count = int(len(all_factors) * min_valid_ratio)
            
            # é¢„åˆ†é…æ•°ç»„
            date_factor_matrix = np.full((len(common_stocks), len(all_factors)), np.nan)
            date_returns = np.full(len(common_stocks), np.nan)
            
            for idx, stock in enumerate(common_stocks):
                # æå–å› å­å€¼
                for j, factor_name in enumerate(all_factors):
                    if factor_name in factor_values_dict:
                        value = factor_values_dict[factor_name].loc[stock]
                        if pd.isna(value):
                            date_factor_matrix[idx, j] = factor_means[factor_name]
                        else:
                            date_factor_matrix[idx, j] = value
                    else:
                        date_factor_matrix[idx, j] = 0.0
                
                # æå–æ”¶ç›Šç‡
                return_value = future_returns.loc[stock]
                if not pd.isna(return_value):
                    date_returns[idx] = return_value
            
            # æ‰¹é‡è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
            valid_factor_counts = (~np.isnan(date_factor_matrix)).sum(axis=1)
            valid_return_mask = ~np.isnan(date_returns)
            valid_mask = (valid_factor_counts >= min_valid_count) & valid_return_mask
            
            if valid_mask.sum() > 0:
                valid_factors = date_factor_matrix[valid_mask]
                valid_returns = date_returns[valid_mask]
                valid_stocks = [common_stocks[i] for i in range(len(common_stocks)) if valid_mask[i]]
                
                X_list.extend(valid_factors.tolist())
                y_list.extend(valid_returns.tolist())
                stock_info.extend([
                    {'stock': stock, 'date': date, 'future_date': future_date}
                    for stock in valid_stocks
                ])
        
        if len(X_list) == 0:
            print("âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            print(f"   å› å­æ•°é‡: {len(all_factors)}")
            print(f"   æ—¥æœŸæ•°é‡: {len(dates)}")
            print("   å¯èƒ½åŸå› ï¼š")
            print("   1. å› å­æ•°æ®å’Œæ”¶ç›Šç‡æ•°æ®çš„æ—¥æœŸ/è‚¡ç¥¨ä¸åŒ¹é…")
            print("   2. æ‰€æœ‰å› å­çš„å€¼éƒ½ç¼ºå¤±")
            print("   3. æˆåˆ†è‚¡è¿‡æ»¤åæ²¡æœ‰å‰©ä½™çš„è‚¡ç¥¨")
            print("   4. æœ‰æ•ˆå› å­æ¯”ä¾‹è¿‡ä½ï¼ˆè¦æ±‚è‡³å°‘30%çš„å› å­æœ‰å€¼ï¼‰")
            
            # æ·»åŠ è¯Šæ–­ä¿¡æ¯
            if len(dates) > 0:
                sample_date = dates[0]
                print(f"\n   è¯Šæ–­ä¿¡æ¯ï¼ˆä»¥æ—¥æœŸ {sample_date} ä¸ºä¾‹ï¼‰ï¼š")
                if sample_date in self.return_data.columns:
                    return_stocks = set(self.return_data[sample_date].dropna().index)
                    print(f"     æ”¶ç›Šç‡æ•°æ®æœ‰æ•ˆè‚¡ç¥¨æ•°: {len(return_stocks)}")
                    
                    factor_stocks_sets = []
                    for factor_name in all_factors[:5]:  # åªæ£€æŸ¥å‰5ä¸ªå› å­
                        if factor_name in self.factor_data:
                            factor_df = self.factor_data[factor_name]
                            if sample_date in factor_df.columns:
                                factor_stocks = set(factor_df[sample_date].dropna().index)
                                factor_stocks_sets.append(factor_stocks)
                                print(f"     {factor_name} æœ‰æ•ˆè‚¡ç¥¨æ•°: {len(factor_stocks)}")
                    
                    if factor_stocks_sets:
                        common_stocks = return_stocks
                        for fs in factor_stocks_sets:
                            common_stocks = common_stocks & fs
                        print(f"     å‰5ä¸ªå› å­ä¸æ”¶ç›Šç‡çš„å…¬å…±è‚¡ç¥¨æ•°: {len(common_stocks)}")
                        
                        if self.constituent_manager is not None:
                            constituents = self.constituent_manager.get_constituents_by_date(sample_date)
                            final_stocks = common_stocks & set(constituents)
                            print(f"     æˆåˆ†è‚¡è¿‡æ»¤åå‰©ä½™è‚¡ç¥¨æ•°: {len(final_stocks)}")
            
            return np.array([]).reshape(0, len(all_factors)), np.array([]), []
        
        X = np.array(X_list)
        y = np.array(y_list)
        
        # æ£€æŸ¥Xçš„ç»´åº¦
        if len(X.shape) == 1:
            # å¦‚æœæ˜¯ä¸€ç»´æ•°ç»„ï¼Œè¯´æ˜æ‰€æœ‰æ ·æœ¬çš„å› å­æ•°é‡ä¸ä¸€è‡´ï¼Œè¿™æ˜¯ä¸åº”è¯¥å‘ç”Ÿçš„
            print(f"âš ï¸  è­¦å‘Šï¼šæ•°æ®æ ¼å¼å¼‚å¸¸ï¼ŒXçš„å½¢çŠ¶ä¸º {X.shape}")
            print(f"   é¢„æœŸçš„ç‰¹å¾æ•°: {len(all_factors)}")
            return np.array([]).reshape(0, len(all_factors)), np.array([]), []
        
        if X.shape[1] != len(all_factors):
            print(f"âš ï¸  è­¦å‘Šï¼šç‰¹å¾æ•°ä¸åŒ¹é…ï¼Œé¢„æœŸ {len(all_factors)}ï¼Œå®é™… {X.shape[1]}")
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {X.shape[1]}")
        return X, y, stock_info
    
    def _load_factor_analysis_results(self, results_dir='./results/'):
        """
        åŠ è½½å› å­åˆ†æç»“æœï¼ˆICç»Ÿè®¡ã€å•è°ƒæ€§ã€å¤šç©ºæ”¶ç›Šç­‰ï¼‰
        
        Parameters:
        -----------
        results_dir : str, ç»“æœæ–‡ä»¶ç›®å½•
        
        Returns:
        --------
        pd.DataFrame : æ‰€æœ‰å› å­çš„åˆ†æç»“æœæ±‡æ€»
        dict : å› å­æ–¹å‘å­—å…¸ï¼Œkeyä¸ºå› å­åï¼Œvalueä¸º'positive'/'negative'æˆ–None
        """
        results_dir = os.path.abspath(results_dir)
        
        # å°è¯•åŠ è½½æ±‡æ€»æ–‡ä»¶
        summary_file = os.path.join(results_dir, 'all_factors_IC_summary.csv')
        if os.path.exists(summary_file):
            print(f"ğŸ“‚ åŠ è½½å› å­åˆ†ææ±‡æ€»: {summary_file}")
            ic_summary = pd.read_csv(summary_file, index_col=0, encoding='utf-8-sig')
            
            # æ„å»ºå› å­æ–¹å‘å­—å…¸
            factor_directions = {}
            for factor_name in ic_summary.index:
                direction = ic_summary.loc[factor_name, 'Direction']
                if pd.notna(direction) and direction != '':
                    factor_directions[factor_name] = str(direction).lower()
                else:
                    # å¦‚æœæ²¡æœ‰æ–¹å‘ä¿¡æ¯ï¼Œæ ¹æ®ICå‡å€¼åˆ¤æ–­
                    ic_mean = ic_summary.loc[factor_name, 'IC_Mean']
                    if pd.notna(ic_mean):
                        factor_directions[factor_name] = 'positive' if ic_mean > 0 else 'negative'
                    else:
                        factor_directions[factor_name] = None
            
            print(f"âœ… åŠ è½½äº† {len(ic_summary)} ä¸ªå› å­çš„åˆ†æç»“æœ")
            return ic_summary, factor_directions
        
        # å¦‚æœæ²¡æœ‰æ±‡æ€»æ–‡ä»¶ï¼Œå°è¯•åŠ è½½å•ä¸ªæ–‡ä»¶
        print(f"âš ï¸  æœªæ‰¾åˆ°æ±‡æ€»æ–‡ä»¶ï¼Œå°è¯•åŠ è½½å•ä¸ªå› å­åˆ†æç»“æœ...")
        ic_stats_list = []
        factor_directions = {}
        
        for factor_name in self.factor_data.keys():
            stats_file = os.path.join(results_dir, f'{factor_name}_IC_stats.csv')
            if os.path.exists(stats_file):
                try:
                    stats_df = pd.read_csv(stats_file, index_col=0, encoding='utf-8-sig')
                    if factor_name in stats_df.index:
                        ic_stats_list.append(stats_df.loc[factor_name])
                        
                        # æå–æ–¹å‘ä¿¡æ¯
                        direction = stats_df.loc[factor_name, 'Direction']
                        if pd.notna(direction) and direction != '':
                            factor_directions[factor_name] = str(direction).lower()
                        else:
                            ic_mean = stats_df.loc[factor_name, 'IC_Mean']
                            if pd.notna(ic_mean):
                                factor_directions[factor_name] = 'positive' if ic_mean > 0 else 'negative'
                            else:
                                factor_directions[factor_name] = None
                except Exception as e:
                    print(f"   âš ï¸  {factor_name}: åŠ è½½å¤±è´¥ - {e}")
        
        if len(ic_stats_list) > 0:
            ic_summary = pd.DataFrame(ic_stats_list)
            print(f"âœ… ä»å•ä¸ªæ–‡ä»¶åŠ è½½äº† {len(ic_summary)} ä¸ªå› å­çš„åˆ†æç»“æœ")
            return ic_summary, factor_directions
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å› å­åˆ†æç»“æœæ–‡ä»¶")
            return pd.DataFrame(), {}
    
    def model1_factor_selection_equal_weight(self, 
                                             ml_min_factors=2, ml_max_factors=3,
                                             price_min_factors=3, price_max_factors=5,
                                             ic_threshold=0.02, correlation_threshold=0.7,
                                             train_ratio=0.8,
                                             results_dir='./results/',
                                             use_analysis_results=True):
        """
        æ¨¡å‹1ï¼šå› å­ç­›é€‰ç­‰æƒåˆæˆæ³•ï¼ˆåˆ†ç±»ç­›é€‰ï¼‰
        
        Parameters:
        -----------
        ml_min_factors : int, æœºå™¨å­¦ä¹ å› å­æœ€å°‘ä¿ç•™æ•°ï¼ˆé»˜è®¤2ï¼‰
        ml_max_factors : int, æœºå™¨å­¦ä¹ å› å­æœ€å¤šä¿ç•™æ•°ï¼ˆé»˜è®¤3ï¼‰
        price_min_factors : int, é‡ä»·å› å­æœ€å°‘ä¿ç•™æ•°ï¼ˆé»˜è®¤3ï¼‰
        price_max_factors : int, é‡ä»·å› å­æœ€å¤šä¿ç•™æ•°ï¼ˆé»˜è®¤5ï¼‰
        ic_threshold : float, ICé˜ˆå€¼ï¼ˆç»å¯¹å€¼ï¼‰ï¼Œä½äºæ­¤å€¼çš„å› å­å°†è¢«å‰”é™¤
        correlation_threshold : float, ç›¸å…³æ€§é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„å› å­å¯¹å°†è¢«å‰”é™¤å…¶ä¸­ä¸€ä¸ª
        train_ratio : float, è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.8ï¼Œå‰80%ç”¨äºè®­ç»ƒï¼Œå20%ç”¨äºæµ‹è¯•ï¼‰
        
        Returns:
        --------
        pd.DataFrame : æ¨¡å‹1çš„é¢„æµ‹å€¼ï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        """
        print("\n" + "="*60)
        print("æ¨¡å‹1ï¼šå› å­ç­›é€‰ç­‰æƒåˆæˆæ³•ï¼ˆåˆ†ç±»ç­›é€‰ï¼‰")
        print("="*60)
        
        # åˆ†ç±»å› å­ï¼šæœºå™¨å­¦ä¹ å› å­ vs é‡ä»·å› å­
        ml_keywords = ['GRU', 'TRANSFORMER', 'SVM', 'LIGHTGBM', 'LGB', 'RF', 'RANDOM']
        ml_factors = []
        price_factors = []
        
        for factor_name in self.factor_data.keys():
            is_ml = any(keyword in factor_name.upper() for keyword in ml_keywords)
            if is_ml:
                ml_factors.append(factor_name)
            else:
                price_factors.append(factor_name)
        
        print(f"\nå› å­åˆ†ç±»ï¼š")
        print(f"  æœºå™¨å­¦ä¹ å› å­: {len(ml_factors)} ä¸ª")
        print(f"  é‡ä»·å› å­: {len(price_factors)} ä¸ª")
        
        # åŠ è½½å› å­åˆ†æç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
        analysis_summary = pd.DataFrame()
        factor_directions = {}
        
        if use_analysis_results:
            analysis_summary, factor_directions = self._load_factor_analysis_results(results_dir)
            if not analysis_summary.empty:
                print(f"âœ… æˆåŠŸåŠ è½½ {len(analysis_summary)} ä¸ªå› å­çš„åˆ†æç»“æœ")
                print(f"   åŒ…å«æŒ‡æ ‡: IC, IR, å•è°ƒæ€§, å¤šç©ºæ”¶ç›Šç­‰")
        
        # è®¡ç®—æ‰€æœ‰å› å­çš„ICï¼ˆåªä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼Œå¹¶è¡Œä¼˜åŒ–ï¼‰
        # å¦‚æœå·²æœ‰åˆ†æç»“æœï¼Œä¼˜å…ˆä½¿ç”¨ï¼›å¦åˆ™é‡æ–°è®¡ç®—
        print("\nè®¡ç®—å› å­ICï¼ˆä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼‰...")
        dates = sorted(self.return_data.columns)
        train_end_idx = int(len(dates) * train_ratio)
        train_dates = dates[:train_end_idx]
        
        # ä¸´æ—¶ä¿å­˜åŸå§‹return_dataï¼Œä½¿ç”¨è®­ç»ƒé›†æ•°æ®è®¡ç®—IC
        original_return_data = self.return_data
        self.return_data = self.return_data[train_dates]
        
        ic_stats = {}
        factor_names = list(self.factor_data.keys())
        
        # å¦‚æœå·²æœ‰åˆ†æç»“æœï¼Œç›´æ¥ä½¿ç”¨ï¼ˆä½†éœ€è¦ç¡®ä¿æ˜¯è®­ç»ƒé›†çš„æ•°æ®ï¼‰
        # ä¸ºäº†å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ä»ç„¶é‡æ–°è®¡ç®—ICï¼ˆä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼‰
        if JOBLIB_AVAILABLE and len(factor_names) > 5:
            # å¹¶è¡Œè®¡ç®—IC
            ic_results = Parallel(n_jobs=-1, backend='threading')(
                delayed(self._calculate_IC_simple)(factor_name)
                for factor_name in tqdm(factor_names, desc="è®¡ç®—IC")
            )
            
            for factor_name, ic_series in zip(factor_names, ic_results):
                if len(ic_series) > 0:
                    ic_mean = ic_series.mean()
                    ic_std = ic_series.std()
                    ir = ic_mean / (ic_std + 1e-8) if ic_std > 1e-8 else 0
                    ic_stats[factor_name] = {
                        'IC_Mean': ic_mean,
                        'IC_Std': ic_std,
                        'IR': ir,
                        'IC_AbsMean': abs(ic_mean)
                    }
                    
                    # å¦‚æœåˆ†æç»“æœä¸­æœ‰æ›´å¤šä¿¡æ¯ï¼Œè¡¥å……è¿›æ¥
                    if not analysis_summary.empty and factor_name in analysis_summary.index:
                        row = analysis_summary.loc[factor_name]
                        ic_stats[factor_name]['Is_Monotonic'] = row.get('Is_Monotonic', False)
                        ic_stats[factor_name]['Long_Short_Return'] = row.get('Long_Short_Return', np.nan)
                        ic_stats[factor_name]['Long_Short_Annual_Return'] = row.get('Long_Short_Annual_Return', np.nan)
                        ic_stats[factor_name]['Long_Short_Sharpe'] = row.get('Long_Short_Sharpe', np.nan)
                        ic_stats[factor_name]['RankIR'] = row.get('RankIR', np.nan)
        else:
            # ä¸²è¡Œè®¡ç®—IC
            for factor_name in tqdm(factor_names, desc="è®¡ç®—IC") if TQDM_AVAILABLE else factor_names:
                ic_series = self._calculate_IC_simple(factor_name)
                if len(ic_series) > 0:
                    ic_mean = ic_series.mean()
                    ic_std = ic_series.std()
                    ir = ic_mean / (ic_std + 1e-8) if ic_std > 1e-8 else 0
                    ic_stats[factor_name] = {
                        'IC_Mean': ic_mean,
                        'IC_Std': ic_std,
                        'IR': ir,
                        'IC_AbsMean': abs(ic_mean)
                    }
                    
                    # å¦‚æœåˆ†æç»“æœä¸­æœ‰æ›´å¤šä¿¡æ¯ï¼Œè¡¥å……è¿›æ¥
                    if not analysis_summary.empty and factor_name in analysis_summary.index:
                        row = analysis_summary.loc[factor_name]
                        ic_stats[factor_name]['Is_Monotonic'] = row.get('Is_Monotonic', False)
                        ic_stats[factor_name]['Long_Short_Return'] = row.get('Long_Short_Return', np.nan)
                        ic_stats[factor_name]['Long_Short_Annual_Return'] = row.get('Long_Short_Annual_Return', np.nan)
                        ic_stats[factor_name]['Long_Short_Sharpe'] = row.get('Long_Short_Sharpe', np.nan)
                        ic_stats[factor_name]['RankIR'] = row.get('RankIR', np.nan)
        
        # æ¢å¤åŸå§‹return_data
        self.return_data = original_return_data
        
        # å¦‚æœfactor_directionsä¸ºç©ºï¼Œæ ¹æ®ICå‡å€¼æ¨æ–­æ–¹å‘
        if not factor_directions:
            print("   æ ¹æ®ICå‡å€¼æ¨æ–­å› å­æ–¹å‘...")
            for factor_name in factor_names:
                if factor_name in ic_stats:
                    ic_mean = ic_stats[factor_name].get('IC_Mean', 0)
                    factor_directions[factor_name] = 'positive' if ic_mean > 0 else 'negative'
        
        # åˆ†åˆ«ç­›é€‰æœºå™¨å­¦ä¹ å› å­å’Œé‡ä»·å› å­ï¼ˆåŸºäºåˆ†æç»“æœä¼˜åŒ–ï¼‰
        def select_factors_by_category(factor_list, min_count, max_count, category_name):
            """ä»æŒ‡å®šç±»åˆ«ä¸­ç­›é€‰å› å­ï¼ˆåŸºäºICã€IRã€å•è°ƒæ€§ã€å¤šç©ºæ”¶ç›Šç­‰ï¼‰"""
            category_factors = [f for f in factor_list if f in ic_stats]
            if len(category_factors) == 0:
                print(f"\n  {category_name}: æ²¡æœ‰å¯ç”¨çš„å› å­")
                return []
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆè€ƒè™‘å¤šä¸ªæŒ‡æ ‡ï¼‰
            def calculate_score(factor_name, stats):
                """è®¡ç®—å› å­ç»¼åˆå¾—åˆ†"""
                score = 0.0
                
                # ICç»å¯¹å€¼ï¼ˆæƒé‡0.3ï¼‰
                score += abs(stats.get('IC_Mean', 0)) * 0.3
                
                # IRï¼ˆæƒé‡0.3ï¼‰
                ir = stats.get('IR', 0)
                if pd.notna(ir):
                    score += abs(ir) * 0.3
                
                # å•è°ƒæ€§ï¼ˆæƒé‡0.2ï¼‰- å•è°ƒå› å­ä¼˜å…ˆ
                is_monotonic = stats.get('Is_Monotonic', False)
                if is_monotonic:
                    score += 0.2
                
                # å¤šç©ºå¹´åŒ–æ”¶ç›Šï¼ˆæƒé‡0.1ï¼‰
                annual_return = stats.get('Long_Short_Annual_Return', np.nan)
                if pd.notna(annual_return) and annual_return > 0:
                    score += min(annual_return / 100, 0.1)  # å½’ä¸€åŒ–åˆ°0.1
                
                # å¤šç©ºå¤æ™®æ¯”ç‡ï¼ˆæƒé‡0.1ï¼‰
                sharpe = stats.get('Long_Short_Sharpe', np.nan)
                if pd.notna(sharpe) and sharpe > 0:
                    score += min(sharpe / 2, 0.1)  # å½’ä¸€åŒ–åˆ°0.1
                
                return score
            
            # è®¡ç®—æ¯ä¸ªå› å­çš„å¾—åˆ†
            factor_scores = []
            for factor_name in category_factors:
                stats = ic_stats[factor_name]
                score = calculate_score(factor_name, stats)
                factor_scores.append((factor_name, stats, score))
            
            # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
            sorted_category = sorted(factor_scores, key=lambda x: x[2], reverse=True)
            
            print(f"\n  {category_name} ç»¼åˆè¯„åˆ†ï¼ˆæŒ‰å¾—åˆ†æ’åºï¼‰:")
            for i, (factor_name, stats, score) in enumerate(sorted_category[:min(10, len(sorted_category))]):
                ic_mean = stats.get('IC_Mean', 0)
                ir = stats.get('IR', 0)
                is_monotonic = stats.get('Is_Monotonic', False)
                annual_return = stats.get('Long_Short_Annual_Return', np.nan)
                print(f"    {i+1}. {factor_name}: å¾—åˆ†={score:.4f}, IC={ic_mean:.4f}, IR={ir:.4f}, "
                      f"å•è°ƒæ€§={'æ˜¯' if is_monotonic else 'å¦'}, "
                      f"å¹´åŒ–æ”¶ç›Š={annual_return:.2f}%" if pd.notna(annual_return) else f"å¹´åŒ–æ”¶ç›Š=NaN")
            
            # åˆæ­¥ç­›é€‰ï¼šå‰”é™¤ICè¾ƒä½çš„å› å­
            selected = []
            for factor_name, stats, score in sorted_category:
                if abs(stats.get('IC_Mean', 0)) >= ic_threshold:
                    selected.append(factor_name)
            
            print(f"    åˆæ­¥ç­›é€‰å: {len(selected)} ä¸ª")
            
            # å‰”é™¤é«˜ç›¸å…³æ€§å› å­å¯¹
            if len(selected) > 1:
                corr_matrix = self._calculate_factor_correlation(selected)
                factors_to_remove = set()
                
                for i, factor1 in enumerate(selected):
                    if factor1 in factors_to_remove:
                        continue
                    for j, factor2 in enumerate(selected[i+1:], start=i+1):
                        if factor2 in factors_to_remove:
                            continue
                        if factor1 in corr_matrix.index and factor2 in corr_matrix.columns:
                            corr = abs(corr_matrix.loc[factor1, factor2])
                            if corr > correlation_threshold:
                                # ä¿ç•™å¾—åˆ†æ›´é«˜çš„å› å­
                                score1 = next((s for f, _, s in sorted_category if f == factor1), 0)
                                score2 = next((s for f, _, s in sorted_category if f == factor2), 0)
                                if score1 < score2:
                                    factors_to_remove.add(factor1)
                                else:
                                    factors_to_remove.add(factor2)
                
                selected = [f for f in selected if f not in factors_to_remove]
                print(f"    å‰”é™¤é«˜ç›¸å…³æ€§å: {len(selected)} ä¸ª")
            
            # ç¡®ä¿æ•°é‡åœ¨åˆç†èŒƒå›´å†…
            if len(selected) > max_count:
                selected = [f for f, _, _ in sorted_category if f in selected][:max_count]
            elif len(selected) < min_count:
                # å¦‚æœç­›é€‰åå› å­å¤ªå°‘ï¼Œè‡³å°‘ä¿ç•™å¾—åˆ†æœ€é«˜çš„min_countä¸ª
                selected = [f for f, _, _ in sorted_category[:min_count]]
            
            return selected
        
        # ç­›é€‰æœºå™¨å­¦ä¹ å› å­
        selected_ml_factors = select_factors_by_category(
            ml_factors, ml_min_factors, ml_max_factors, "æœºå™¨å­¦ä¹ å› å­"
        )
        
        # ç­›é€‰é‡ä»·å› å­
        selected_price_factors = select_factors_by_category(
            price_factors, price_min_factors, price_max_factors, "é‡ä»·å› å­"
        )
        
        # åˆå¹¶é€‰ä¸­çš„å› å­
        selected_factors = selected_ml_factors + selected_price_factors
        
        print(f"\næœ€ç»ˆé€‰æ‹©çš„å› å­ï¼ˆ{len(selected_factors)}ä¸ªï¼‰:")
        if selected_ml_factors:
            print(f"  æœºå™¨å­¦ä¹ å› å­ï¼ˆ{len(selected_ml_factors)}ä¸ªï¼‰:")
            for i, factor_name in enumerate(selected_ml_factors, 1):
                direction = factor_directions.get(factor_name, 'unknown')
                direction_str = 'æ­£å‘' if direction == 'positive' else 'è´Ÿå‘' if direction == 'negative' else 'æœªçŸ¥'
                print(f"    {i}. {factor_name} (æ–¹å‘: {direction_str})")
        if selected_price_factors:
            print(f"  é‡ä»·å› å­ï¼ˆ{len(selected_price_factors)}ä¸ªï¼‰:")
            for i, factor_name in enumerate(selected_price_factors, 1):
                direction = factor_directions.get(factor_name, 'unknown')
                direction_str = 'æ­£å‘' if direction == 'positive' else 'è´Ÿå‘' if direction == 'negative' else 'æœªçŸ¥'
                print(f"    {i}. {factor_name} (æ–¹å‘: {direction_str})")
        
        # ç­‰æƒåˆæˆï¼ˆåªå¯¹è®­ç»ƒé›†æ—¥æœŸè¿›è¡Œé¢„æµ‹ï¼Œæµ‹è¯•é›†ç•™ç©ºç”¨äºçº¯è¢‹å¤–è§‚æµ‹ï¼‰
        # è€ƒè™‘å› å­æ–¹å‘ï¼šè´Ÿå‘å› å­éœ€è¦å–å
        print("\nè¿›è¡Œç­‰æƒåˆæˆï¼ˆè€ƒè™‘å› å­æ–¹å‘ï¼‰...")
        dates = sorted(self.return_data.columns)
        train_end_idx = int(len(dates) * train_ratio)
        train_dates = dates[:train_end_idx]
        test_dates = dates[train_end_idx:]
        
        print(f"  è®­ç»ƒé›†æ—¥æœŸ: {len(train_dates)} ä¸ª ({train_dates[0]} è‡³ {train_dates[-1]})")
        print(f"  æµ‹è¯•é›†æ—¥æœŸ: {len(test_dates)} ä¸ª ({test_dates[0]} è‡³ {test_dates[-1]}) - çº¯è¢‹å¤–è§‚æµ‹")
        
        stocks = sorted(self.return_data.index)
        
        prediction = pd.DataFrame(index=stocks, columns=dates)
        prediction[:] = np.nan
        
        # åªå¯¹è®­ç»ƒé›†æ—¥æœŸè¿›è¡Œé¢„æµ‹ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰
        date_iterator = tqdm(train_dates, desc="ç­‰æƒåˆæˆ") if TQDM_AVAILABLE else train_dates
        
        # é¢„è®¡ç®—æˆåˆ†è‚¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        constituents_cache = {}
        if self.constituent_manager is not None:
            for date in train_dates:
                constituents_cache[date] = set(self.constituent_manager.get_constituents_by_date(date))
        
        for date in date_iterator:
            # è·å–æ‰€æœ‰é€‰ä¸­å› å­åœ¨è¯¥æ—¥æœŸçš„å€¼ï¼ˆè€ƒè™‘æ–¹å‘ï¼‰
            factor_values_list = []
            factor_directions_list = []  # è®°å½•æ¯ä¸ªå› å­çš„æ–¹å‘
            
            for factor_name in selected_factors:
                if factor_name in self.factor_data:
                    factor_df = self.factor_data[factor_name]
                    if date in factor_df.columns:
                        factor_values = factor_df[date].copy()
                        
                        # æ ¹æ®å› å­æ–¹å‘å†³å®šæ˜¯å¦å–å
                        direction = factor_directions.get(factor_name, None)
                        if direction == 'negative':
                            # è´Ÿå‘å› å­ï¼šå› å­å€¼ä½æ”¶ç›Šé«˜ï¼Œéœ€è¦å–å
                            factor_values = -factor_values
                        
                        factor_values_list.append(factor_values)
                        factor_directions_list.append(direction)
            
            if len(factor_values_list) == 0:
                continue
            
            # å¯¹é½æ‰€æœ‰å› å­ï¼ˆå‘é‡åŒ–ï¼‰
            common_stocks = set(stocks)
            for factor_values in factor_values_list:
                common_stocks = common_stocks & set(factor_values.index)
            
            # å¦‚æœæä¾›äº†æˆåˆ†è‚¡ç®¡ç†å™¨ï¼Œåªä½¿ç”¨æˆåˆ†è‚¡
            if self.constituent_manager is not None:
                if date in constituents_cache:
                    common_stocks = common_stocks & constituents_cache[date]
            
            if len(common_stocks) == 0:
                continue
            
            common_stocks = sorted(list(common_stocks))
            
            # å‘é‡åŒ–æ„å»ºå› å­çŸ©é˜µ
            factor_matrix = np.full((len(common_stocks), len(factor_values_list)), np.nan)
            for j, factor_values in enumerate(factor_values_list):
                factor_matrix[:, j] = factor_values.loc[common_stocks].values
            
            # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ï¼ˆæ‰€æœ‰å› å­éƒ½æœ‰å€¼ï¼‰
            valid_mask = ~np.isnan(factor_matrix).any(axis=1)
            
            if valid_mask.sum() == 0:
                continue
            
            factor_matrix_valid = factor_matrix[valid_mask]
            valid_stocks = [common_stocks[i] for i in range(len(common_stocks)) if valid_mask[i]]
            
            # æˆªé¢æ ‡å‡†åŒ–ï¼ˆå‘é‡åŒ–ï¼‰
            factor_matrix_std = (factor_matrix_valid - factor_matrix_valid.mean(axis=0)) / (factor_matrix_valid.std(axis=0) + 1e-8)
            
            # ç­‰æƒåˆæˆï¼ˆå‘é‡åŒ–ï¼‰- æ­¤æ—¶æ‰€æœ‰å› å­éƒ½å·²ç»Ÿä¸€ä¸ºæ­£å‘
            combined_signal = factor_matrix_std.mean(axis=1)
            
            # æ‰¹é‡ä¿å­˜é¢„æµ‹å€¼
            prediction.loc[valid_stocks, date] = combined_signal
        
        self.model1_prediction = prediction
        self.model1_factor_directions = {f: factor_directions.get(f, None) for f in selected_factors}
        print(f"âœ… æ¨¡å‹1å®Œæˆï¼Œé¢„æµ‹å€¼å½¢çŠ¶: {prediction.shape}")
        print(f"   å·²è€ƒè™‘å› å­æ–¹å‘ï¼šæ­£å‘å› å­ç›´æ¥ä½¿ç”¨ï¼Œè´Ÿå‘å› å­å·²å–å")
        return prediction
    
    def model2_mlp(self, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2,
                   hidden_dims=[128, 64, 32], dropout=0.3, 
                   batch_size=256, epochs=100, patience=10, lr=0.001):
        """
        æ¨¡å‹2ï¼šMLPæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹
        
        Parameters:
        -----------
        train_ratio : float, è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.6ï¼Œå³3:1:1ä¸­çš„3/5ï¼‰
        val_ratio : float, éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰
        test_ratio : float, æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰
        hidden_dims : list, éšè—å±‚ç»´åº¦
        dropout : float, Dropoutæ¯”ä¾‹
        batch_size : int, æ‰¹æ¬¡å¤§å°
        epochs : int, è®­ç»ƒè½®æ•°
        patience : int, æ—©åœè€å¿ƒå€¼
        lr : float, å­¦ä¹ ç‡
        
        Returns:
        --------
        pd.DataFrame : æ¨¡å‹2çš„é¢„æµ‹å€¼ï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        """
        print("\n" + "="*60)
        print("æ¨¡å‹2ï¼šMLPæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹")
        print("="*60)
        
        # å‡†å¤‡æ•°æ®
        X, y, stock_info = self._prepare_training_data(forward_period=1)
        
        if len(X) == 0:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return pd.DataFrame()
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†ï¼ˆ3:1:1ï¼‰
        n_samples = len(X)
        n_train = int(n_samples * train_ratio)
        n_val = int(n_samples * val_ratio)
        
        X_train = X[:n_train]
        y_train = y[:n_train]
        X_val = X[n_train:n_train+n_val]
        y_val = y[n_train:n_train+n_val]
        X_test = X[n_train+n_val:]
        y_test = y[n_train+n_val:]
        stock_info_test = stock_info[n_train+n_val:]
        
        print(f"è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}, æµ‹è¯•é›†: {len(X_test)}")
        
        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)
        
        scaler_y = StandardScaler()
        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = FactorDataset(X_train_scaled, y_train_scaled)
        val_dataset = FactorDataset(X_val_scaled, y_val_scaled)
        test_dataset = FactorDataset(X_test_scaled, y_test_scaled)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨GPUå¦‚æœå¯ç”¨ï¼‰
        input_dim = X_train.shape[1]
        model = MLPModel(input_dim, hidden_dims, dropout).to(DEVICE)
        criterion = CCCLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nå¼€å§‹è®­ç»ƒMLPæ¨¡å‹ï¼ˆä½¿ç”¨ {DEVICE}ï¼‰...")
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        epoch_iterator = tqdm(range(epochs), desc="è®­ç»ƒè¿›åº¦") if TQDM_AVAILABLE else range(epochs)
        
        for epoch in epoch_iterator:
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(DEVICE)
                batch_y = batch_y.to(DEVICE)
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X = batch_X.to(DEVICE)
                    batch_y = batch_y.to(DEVICE)
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            if TQDM_AVAILABLE:
                epoch_iterator.set_postfix({
                    'Train Loss': f'{train_loss:.6f}',
                    'Val Loss': f'{val_loss:.6f}'
                })
            elif (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            # æ—©åœ
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    if TQDM_AVAILABLE:
                        epoch_iterator.close()
                    print(f"æ—©åœäº Epoch {epoch+1}")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # é¢„æµ‹ï¼ˆä½¿ç”¨GPUå¦‚æœå¯ç”¨ï¼‰
        print("\nè¿›è¡Œé¢„æµ‹...")
        model.eval()
        predictions = []
        with torch.no_grad():
            for batch_X, _ in test_loader:
                batch_X = batch_X.to(DEVICE)
                outputs = model(batch_X)
                predictions.append(outputs.cpu().numpy())
        
        predictions = np.concatenate(predictions)
        predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()
        
        # è½¬æ¢ä¸ºå› å­æ ¼å¼
        dates = sorted(self.return_data.columns)
        stocks = sorted(self.return_data.index)
        prediction_df = pd.DataFrame(index=stocks, columns=dates)
        prediction_df[:] = np.nan
        
        for i, info in enumerate(stock_info_test):
            if i < len(predictions) and not np.isnan(predictions[i]):
                stock = info['stock']
                date = info['date']
                if stock in prediction_df.index and date in prediction_df.columns:
                    prediction_df.loc[stock, date] = predictions[i]
        
        self.model2_prediction = prediction_df
        print(f"âœ… æ¨¡å‹2å®Œæˆï¼Œé¢„æµ‹å€¼å½¢çŠ¶: {prediction_df.shape}")
        return prediction_df
    
    def model3_xgboost(self, train_ratio=0.8, test_ratio=0.2,
                      n_estimators=200, max_depth=6, learning_rate=0.1,
                      subsample=0.8, colsample_bytree=0.8):
        """
        æ¨¡å‹3ï¼šXGBoostæ¨¡å‹
        
        Parameters:
        -----------
        train_ratio : float, è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.8ï¼Œå³4:1ä¸­çš„4/5ï¼‰
        test_ratio : float, æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰
        n_estimators : int, æ ‘çš„æ•°é‡
        max_depth : int, æ ‘çš„æœ€å¤§æ·±åº¦
        learning_rate : float, å­¦ä¹ ç‡
        subsample : float, æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
        colsample_bytree : float, ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
        
        Returns:
        --------
        pd.DataFrame : æ¨¡å‹3çš„é¢„æµ‹å€¼ï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        """
        print("\n" + "="*60)
        print("æ¨¡å‹3ï¼šXGBoostæ¨¡å‹")
        print("="*60)
        
        # å‡†å¤‡æ•°æ®
        X, y, stock_info = self._prepare_training_data(forward_period=1)
        
        if len(X) == 0:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return pd.DataFrame()
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†ï¼ˆå‰80%è®­ç»ƒï¼Œå20%æµ‹è¯•ï¼‰
        n_samples = len(X)
        n_train = int(n_samples * train_ratio)
        
        X_train = X[:n_train]
        y_train = y[:n_train]
        X_test = X[n_train:]
        y_test = y[n_train:]
        stock_info_train = stock_info[:n_train]
        stock_info_test = stock_info[n_train:]
        
        print(f"è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)} (çº¯è¢‹å¤–è§‚æµ‹)")
        
        # æ ‡å‡†åŒ–ï¼ˆåªä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼‰
        scaler_X = StandardScaler()
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_test_scaled = scaler_X.transform(X_test)
        
        # åˆ›å»ºXGBoostæ¨¡å‹ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
        print("\nå¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰...")
        
        # ä½¿ç”¨GPUå¦‚æœå¯ç”¨
        tree_method = 'gpu_hist' if torch.cuda.is_available() else 'hist'
        
        model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            random_state=42,
            n_jobs=-1,
            tree_method=tree_method,
            predictor='gpu_predictor' if torch.cuda.is_available() else 'cpu_predictor'
        )
        
        # åªä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
        model.fit(X_train_scaled, y_train, 
                 eval_set=[(X_train_scaled, y_train)],
                 verbose=False)
        
        # å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼ˆçº¯è¢‹å¤–è§‚æµ‹ï¼‰
        print("\nå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼ˆçº¯è¢‹å¤–è§‚æµ‹ï¼‰...")
        predictions_test = model.predict(X_test_scaled)
        
        # è®¡ç®—æµ‹è¯•é›†CCC
        ccc_test = calculate_ccc_numpy(predictions_test, y_test)
        print(f"æµ‹è¯•é›†CCC: {ccc_test:.6f}")
        
        # å¯¹è®­ç»ƒé›†ä¹Ÿè¿›è¡Œé¢„æµ‹ï¼ˆç”¨äºå®Œæ•´æ€§ï¼‰
        predictions_train = model.predict(X_train_scaled)
        ccc_train = calculate_ccc_numpy(predictions_train, y_train)
        print(f"è®­ç»ƒé›†CCC: {ccc_train:.6f}")
        
        # è½¬æ¢ä¸ºå› å­æ ¼å¼ï¼ˆåŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢„æµ‹ï¼‰
        dates = sorted(self.return_data.columns)
        stocks = sorted(self.return_data.index)
        prediction_df = pd.DataFrame(index=stocks, columns=dates)
        prediction_df[:] = np.nan
        
        # ä¿å­˜è®­ç»ƒé›†é¢„æµ‹
        for i, info in enumerate(stock_info_train):
            if i < len(predictions_train) and not np.isnan(predictions_train[i]):
                stock = info['stock']
                date = info['date']
                if stock in prediction_df.index and date in prediction_df.columns:
                    prediction_df.loc[stock, date] = predictions_train[i]
        
        # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹ï¼ˆçº¯è¢‹å¤–è§‚æµ‹ï¼‰
        for i, info in enumerate(stock_info_test):
            if i < len(predictions_test) and not np.isnan(predictions_test[i]):
                stock = info['stock']
                date = info['date']
                if stock in prediction_df.index and date in prediction_df.columns:
                    prediction_df.loc[stock, date] = predictions_test[i]
        
        self.model3_prediction = prediction_df
        print(f"âœ… æ¨¡å‹3å®Œæˆï¼Œé¢„æµ‹å€¼å½¢çŠ¶: {prediction_df.shape}")
        return prediction_df
    
    def combine_models(self):
        """
        ç­‰æƒåˆæˆä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œå¾—åˆ°æœ€ç»ˆäº¤æ˜“ä¿¡å·
        
        Returns:
        --------
        pd.DataFrame : æœ€ç»ˆäº¤æ˜“ä¿¡å·ï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        """
        print("\n" + "="*60)
        print("ç­‰æƒåˆæˆä¸‰ä¸ªæ¨¡å‹")
        print("="*60)
        
        if self.model1_prediction is None or self.model2_prediction is None or self.model3_prediction is None:
            print("âš ï¸  è¯·å…ˆè¿è¡Œä¸‰ä¸ªæ¨¡å‹")
            return pd.DataFrame()
        
        # å¯¹é½ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼
        dates = sorted(set(self.model1_prediction.columns) & 
                      set(self.model2_prediction.columns) & 
                      set(self.model3_prediction.columns))
        stocks = sorted(set(self.model1_prediction.index) & 
                       set(self.model2_prediction.index) & 
                       set(self.model3_prediction.index))
        
        final_signal = pd.DataFrame(index=stocks, columns=dates)
        final_signal[:] = np.nan
        
        # å‘é‡åŒ–åˆæˆï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        pred1_aligned = self.model1_prediction.loc[stocks, dates]
        pred2_aligned = self.model2_prediction.loc[stocks, dates]
        pred3_aligned = self.model3_prediction.loc[stocks, dates]
        
        # å‘é‡åŒ–è®¡ç®—
        valid_mask = ~(pred1_aligned.isna() | pred2_aligned.isna() | pred3_aligned.isna())
        combined = (pred1_aligned + pred2_aligned + pred3_aligned) / 3
        final_signal.loc[stocks, dates] = combined
        final_signal.loc[stocks, dates] = final_signal.loc[stocks, dates].where(valid_mask, np.nan)
        
        self.final_signal = final_signal
        print(f"âœ… æœ€ç»ˆä¿¡å·ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {final_signal.shape}")
        return final_signal
    
    def save_predictions(self, save_dir='./results/'):
        """
        ä¿å­˜ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼å’Œæœ€ç»ˆä¿¡å·
        
        Parameters:
        -----------
        save_dir : str, ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nä¿å­˜é¢„æµ‹ç»“æœåˆ°: {save_dir}")
        
        if self.model1_prediction is not None:
            path1 = os.path.join(save_dir, 'model1_factor_selection_prediction.csv')
            self.model1_prediction.to_csv(path1, encoding='utf-8-sig')
            print(f"  âœ… æ¨¡å‹1é¢„æµ‹å€¼: {path1}")
        
        if self.model2_prediction is not None:
            path2 = os.path.join(save_dir, 'model2_mlp_prediction.csv')
            self.model2_prediction.to_csv(path2, encoding='utf-8-sig')
            print(f"  âœ… æ¨¡å‹2é¢„æµ‹å€¼: {path2}")
        
        if self.model3_prediction is not None:
            path3 = os.path.join(save_dir, 'model3_xgboost_prediction.csv')
            self.model3_prediction.to_csv(path3, encoding='utf-8-sig')
            print(f"  âœ… æ¨¡å‹3é¢„æµ‹å€¼: {path3}")
        
        if self.final_signal is not None:
            path_final = os.path.join(save_dir, 'final_signal.csv')
            self.final_signal.to_csv(path_final, encoding='utf-8-sig')
            print(f"  âœ… æœ€ç»ˆä¿¡å·: {path_final}")
    
    def _calculate_IC_simple(self, factor_name, forward_period=1):
        """ç®€åŒ–çš„ICè®¡ç®—ï¼ˆç”¨äºå› å­ç­›é€‰ï¼‰"""
        if factor_name not in self.factor_data:
            return pd.Series(dtype=float)
        
        factor_df = self.factor_data[factor_name]
        dates = sorted(set(factor_df.columns) & set(self.return_data.columns))
        
        ic_values = []
        ic_dates = []
        
        for i, date in enumerate(dates):
            if i + forward_period >= len(dates):
                break
            
            factor_values = factor_df[date]
            future_date = dates[i + forward_period]
            future_returns = self.return_data[future_date]
            
            common_stocks = set(factor_values.index) & set(future_returns.index)
            
            if self.constituent_manager is not None:
                constituents = self.constituent_manager.get_constituents_by_date(date)
                common_stocks = common_stocks & set(constituents)
            
            factor_aligned = factor_values.loc[list(common_stocks)]
            return_aligned = future_returns.loc[list(common_stocks)]
            
            valid_mask = ~(factor_aligned.isna() | return_aligned.isna())
            if valid_mask.sum() < 10:
                continue
            
            factor_clean = factor_aligned[valid_mask]
            return_clean = return_aligned[valid_mask]
            
            if len(factor_clean) > 1 and factor_clean.std() > 1e-8:
                ic = np.corrcoef(factor_clean, return_clean)[0, 1]
                if not np.isnan(ic):
                    ic_values.append(ic)
                    ic_dates.append(date)
        
        return pd.Series(ic_values, index=ic_dates)
    
    def _calculate_factor_correlation(self, factor_list):
        """è®¡ç®—å› å­ç›¸å…³æ€§çŸ©é˜µ"""
        all_dates = set(self.return_data.columns)
        all_stocks = set(self.return_data.index)
        
        for factor_name in factor_list:
            if factor_name in self.factor_data:
                factor_df = self.factor_data[factor_name]
                if factor_df is not None and not factor_df.empty:
                    all_dates = all_dates & set(factor_df.columns)
                    all_stocks = all_stocks & set(factor_df.index)
        
        factor_values_dict = {}
        for factor_name in factor_list:
            if factor_name not in self.factor_data:
                continue
            factor_df = self.factor_data[factor_name]
            if factor_df is None or factor_df.empty:
                continue
            
            factor_aligned = factor_df.loc[list(all_stocks), list(sorted(all_dates))]
            factor_flat = factor_aligned.values.flatten()
            valid_mask = ~np.isnan(factor_flat)
            if valid_mask.sum() > 0:
                factor_values_dict[factor_name] = factor_flat[valid_mask]
        
        if len(factor_values_dict) == 0:
            return pd.DataFrame()
        
        min_len = min(len(v) for v in factor_values_dict.values())
        factor_aligned_dict = {}
        for factor_name, factor_values in factor_values_dict.items():
            factor_aligned_dict[factor_name] = factor_values[:min_len]
        
        factor_df = pd.DataFrame(factor_aligned_dict)
        return factor_df.corr()


# ==================== ä¸»å‡½æ•° ====================

def _load_single_factor(csv_file, factors_path):
    """åŠ è½½å•ä¸ªå› å­æ–‡ä»¶ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰"""
    factor_name = csv_file.replace('.csv', '')
    factor_path = os.path.join(factors_path, csv_file)
    
    try:
        factor_df = pd.read_csv(factor_path, index_col=0, encoding='utf-8')
        factor_df.columns = pd.to_datetime(factor_df.columns)
        return factor_name, factor_df, None
    except Exception as e:
        return factor_name, None, str(e)


def load_factors_from_directory(factors_path='./factors/'):
    """
    ä»factorsç›®å½•åŠ è½½æ‰€æœ‰å› å­æ•°æ®ï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰
    
    Parameters:
    -----------
    factors_path : str, å› å­æ–‡ä»¶ç›®å½•è·¯å¾„
    
    Returns:
    --------
    dict : å› å­æ•°æ®å­—å…¸ï¼Œkeyä¸ºå› å­åï¼Œvalueä¸ºDataFrameï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
    """
    factors = {}
    factors_path = os.path.abspath(factors_path)
    
    if not os.path.exists(factors_path):
        print(f"âŒ å› å­ç›®å½•ä¸å­˜åœ¨: {factors_path}")
        return factors
    
    print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½å› å­æ•°æ®: {factors_path}")
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(factors_path) if f.endswith('.csv')]
    csv_files.sort()
    
    print(f"   æ‰¾åˆ° {len(csv_files)} ä¸ªå› å­æ–‡ä»¶")
    
    # å¹¶è¡ŒåŠ è½½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if JOBLIB_AVAILABLE and len(csv_files) > 5:
        print("   ä½¿ç”¨å¹¶è¡ŒåŠ è½½...")
        results = Parallel(n_jobs=-1, backend='threading')(
            delayed(_load_single_factor)(csv_file, factors_path) 
            for csv_file in tqdm(csv_files, desc="åŠ è½½å› å­")
        )
        
        for factor_name, factor_df, error in results:
            if error is None:
                factors[factor_name] = factor_df
                print(f"   âœ… {factor_name}: {factor_df.shape}")
            else:
                print(f"   âš ï¸  {factor_name}: åŠ è½½å¤±è´¥ - {error}")
    else:
        # ä¸²è¡ŒåŠ è½½
        for csv_file in tqdm(csv_files, desc="åŠ è½½å› å­"):
            factor_name, factor_df, error = _load_single_factor(csv_file, factors_path)
            if error is None:
                factors[factor_name] = factor_df
                print(f"   âœ… {factor_name}: {factor_df.shape}")
            else:
                print(f"   âš ï¸  {factor_name}: åŠ è½½å¤±è´¥ - {error}")
    
    print(f"\nâœ… å…±åŠ è½½ {len(factors)} ä¸ªå› å­")
    return factors


def load_price_data_and_calculate_returns(data_path='./data/'):
    """
    åŠ è½½ä»·æ ¼æ•°æ®å¹¶è®¡ç®—æ”¶ç›Šç‡
    
    Parameters:
    -----------
    data_path : str, æ•°æ®æ–‡ä»¶ç›®å½•è·¯å¾„
    
    Returns:
    --------
    pd.DataFrame : æ”¶ç›Šç‡æ•°æ®ï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
    """
    data_path = os.path.abspath(data_path)
    price_file = os.path.join(data_path, 'stock_price_data.csv')
    
    if not os.path.exists(price_file):
        print(f"âŒ ä»·æ ¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {price_file}")
        return pd.DataFrame()
    
    print(f"ğŸ“‚ åŠ è½½ä»·æ ¼æ•°æ®: {price_file}")
    
    try:
        price_data = pd.read_csv(price_file, encoding='utf-8-sig')
        price_data['TRADE_DT'] = pd.to_datetime(price_data['TRADE_DT'])
        
        # ç¡®ä¿CLOSE_ADJå­˜åœ¨
        if 'CLOSE_ADJ' not in price_data.columns:
            if 'CLOSE_PRICE' in price_data.columns:
                price_data['CLOSE_ADJ'] = price_data['CLOSE_PRICE']
            else:
                print("âŒ ä»·æ ¼æ•°æ®ä¸­ç¼ºå°‘CLOSE_PRICEæˆ–CLOSE_ADJå­—æ®µ")
                return pd.DataFrame()
        
        # æŒ‰è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸæ’åº
        price_data = price_data.sort_values(['S_INFO_WINDCODE', 'TRADE_DT'])
        
        # è®¡ç®—æ”¶ç›Šç‡
        price_data['RETURN'] = price_data.groupby('S_INFO_WINDCODE')['CLOSE_ADJ'].pct_change()
        
        # è½¬æ¢ä¸ºå®½æ ¼å¼ï¼ˆè‚¡ç¥¨Ã—æ—¥æœŸï¼‰
        return_df = price_data[['S_INFO_WINDCODE', 'TRADE_DT', 'RETURN']].dropna()
        return_wide = return_df.pivot(
            index='S_INFO_WINDCODE',
            columns='TRADE_DT',
            values='RETURN'
        )
        
        print(f"âœ… æ”¶ç›Šç‡æ•°æ®åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {return_wide.shape}")
        return return_wide
    
    except Exception as e:
        print(f"âŒ åŠ è½½ä»·æ ¼æ•°æ®æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()


def load_constituent_manager(data_path='./data/'):
    """
    åŠ è½½æˆåˆ†è‚¡ç®¡ç†å™¨
    
    Parameters:
    -----------
    data_path : str, æ•°æ®æ–‡ä»¶ç›®å½•è·¯å¾„
    
    Returns:
    --------
    ConstituentManager or None
    """
    data_path = os.path.abspath(data_path)
    constituents_file = os.path.join(data_path, 'csi1000_constituents_history.csv')
    
    if not os.path.exists(constituents_file):
        print(f"âš ï¸  æˆåˆ†è‚¡å†å²æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {constituents_file}")
        return None
    
    try:
        from constituent_manager import ConstituentManager
        
        print(f"ğŸ“‚ åŠ è½½æˆåˆ†è‚¡å†å²æ•°æ®: {constituents_file}")
        constituents_history = pd.read_csv(constituents_file, encoding='utf-8-sig')
        constituents_history['S_CON_INDATE'] = pd.to_datetime(constituents_history['S_CON_INDATE'], errors='coerce')
        constituents_history['S_CON_OUTDATE'] = pd.to_datetime(constituents_history['S_CON_OUTDATE'], errors='coerce')
        
        constituent_manager = ConstituentManager(constituents_history)
        print(f"âœ… æˆåˆ†è‚¡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        return constituent_manager
    except Exception as e:
        print(f"âš ï¸  åŠ è½½æˆåˆ†è‚¡ç®¡ç†å™¨æ—¶å‡ºé”™: {e}")
        return None


def main(factors_path='./factors/', data_path='./data/', signal_path='./signal/'):
    """
    ä¸»å‡½æ•°ï¼šåŠ è½½å› å­æ•°æ®ï¼Œè¿è¡Œä¸‰ä¸ªæ¨¡å‹ï¼Œä¿å­˜é¢„æµ‹ç»“æœ
    
    Parameters:
    -----------
    factors_path : str, å› å­æ–‡ä»¶ç›®å½•è·¯å¾„
    data_path : str, æ•°æ®æ–‡ä»¶ç›®å½•è·¯å¾„
    signal_path : str, ä¿¡å·ä¿å­˜ç›®å½•è·¯å¾„
    """
    print("="*80)
    print("ä¸­è¯1000å¤šå› å­åˆæˆ - å› å­åˆæˆæ¨¡å‹")
    print("="*80)
    print()
    
    # 1. åŠ è½½å› å­æ•°æ®
    print("ã€æ­¥éª¤1ã€‘åŠ è½½å› å­æ•°æ®")
    print("-"*80)
    factor_data = load_factors_from_directory(factors_path)
    
    if len(factor_data) == 0:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½•å› å­æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    # 2. åŠ è½½æ”¶ç›Šç‡æ•°æ®
    print("\nã€æ­¥éª¤2ã€‘åŠ è½½æ”¶ç›Šç‡æ•°æ®")
    print("-"*80)
    return_data = load_price_data_and_calculate_returns(data_path)
    
    if return_data.empty:
        print("âŒ æœªåŠ è½½åˆ°æ”¶ç›Šç‡æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    # 3. åŠ è½½æˆåˆ†è‚¡ç®¡ç†å™¨
    print("\nã€æ­¥éª¤3ã€‘åŠ è½½æˆåˆ†è‚¡ç®¡ç†å™¨")
    print("-"*80)
    constituent_manager = load_constituent_manager(data_path)
    
    # 4. åˆå§‹åŒ–å› å­åˆæˆå™¨
    print("\nã€æ­¥éª¤4ã€‘åˆå§‹åŒ–å› å­åˆæˆå™¨")
    print("-"*80)
    combiner = FactorCombiner(
        factor_data=factor_data,
        return_data=return_data,
        constituent_manager=constituent_manager
    )
    
    # 5. è¿è¡Œæ¨¡å‹1ï¼šå› å­ç­›é€‰ç­‰æƒåˆæˆæ³•ï¼ˆåˆ†ç±»ç­›é€‰ï¼ŒåŸºäºåˆ†æç»“æœä¼˜åŒ–ï¼‰
    print("\nã€æ­¥éª¤5ã€‘è¿è¡Œæ¨¡å‹1ï¼šå› å­ç­›é€‰ç­‰æƒåˆæˆæ³•ï¼ˆåˆ†ç±»ç­›é€‰ï¼ŒåŸºäºåˆ†æç»“æœï¼‰")
    print("-"*80)
    
    # ç¡®å®šç»“æœç›®å½•ï¼ˆå¦‚æœå­˜åœ¨resultsç›®å½•ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨signalç›®å½•çš„çˆ¶ç›®å½•ï¼‰
    results_dir = './results/'
    if not os.path.exists(results_dir):
        # å°è¯•ä½¿ç”¨signalç›®å½•çš„çˆ¶ç›®å½•ä¸‹çš„results
        parent_results = os.path.join(os.path.dirname(signal_path.rstrip('/')), 'results')
        if os.path.exists(parent_results):
            results_dir = parent_results + '/'
    
    model1_pred = combiner.model1_factor_selection_equal_weight(
        ml_min_factors=2,
        ml_max_factors=3,
        price_min_factors=3,
        price_max_factors=5,
        ic_threshold=0.02,
        correlation_threshold=0.7,
        train_ratio=0.8,
        results_dir=results_dir,
        use_analysis_results=True
    )
    
    # 6. è¿è¡Œæ¨¡å‹2ï¼šMLPæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹
    print("\nã€æ­¥éª¤6ã€‘è¿è¡Œæ¨¡å‹2ï¼šMLPæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹")
    print("-"*80)
    model2_pred = combiner.model2_mlp(
        train_ratio=0.6,
        val_ratio=0.2,
        test_ratio=0.2,
        hidden_dims=[128, 64, 32],
        dropout=0.3,
        batch_size=256,
        epochs=100,
        patience=10,
        lr=0.001
    )
    
    # 7. è¿è¡Œæ¨¡å‹3ï¼šXGBoostæ¨¡å‹
    print("\nã€æ­¥éª¤7ã€‘è¿è¡Œæ¨¡å‹3ï¼šXGBoostæ¨¡å‹")
    print("-"*80)
    model3_pred = combiner.model3_xgboost(
        train_ratio=0.8,
        test_ratio=0.2,
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8
    )
    
    # 8. ç­‰æƒåˆæˆä¸‰ä¸ªæ¨¡å‹
    print("\nã€æ­¥éª¤8ã€‘ç­‰æƒåˆæˆä¸‰ä¸ªæ¨¡å‹")
    print("-"*80)
    final_signal = combiner.combine_models()
    
    # 9. ä¿å­˜é¢„æµ‹ç»“æœ
    print("\nã€æ­¥éª¤9ã€‘ä¿å­˜é¢„æµ‹ç»“æœ")
    print("-"*80)
    os.makedirs(signal_path, exist_ok=True)
    combiner.save_predictions(save_dir=signal_path)
    
    print("\n" + "="*80)
    print("âœ… å› å­åˆæˆå®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    import sys
    
    # è®¾ç½®é»˜è®¤è·¯å¾„ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    factors_path = 'd:/programme/vscode_c/courses/Software Enginerring/factors/'
    data_path = 'd:/programme/vscode_c/courses/Software Enginerring/data/'
    signal_path = 'd:/programme/vscode_c/courses/Software Enginerring/signal/'
    
    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        factors_path = sys.argv[1]
    if len(sys.argv) > 2:
        data_path = sys.argv[2]
    if len(sys.argv) > 3:
        signal_path = sys.argv[3]
    
    main(factors_path=factors_path, data_path=data_path, signal_path=signal_path)
